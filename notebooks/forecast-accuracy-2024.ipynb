{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa97e7ba",
   "metadata": {},
   "source": [
    "# Executive Budgeting + Forecasting — Forecast Accuracy (Monthly)\n",
    "**Provided Forecast vs Seasonal-Naive baseline**  \n",
    "*Project MVP: quantify how the current forecast performs against a naive yardstick.*\n",
    "\n",
    "---\n",
    "\n",
    "## Parameters\n",
    "- **TEST_YEAR:** `2024` \n",
    "- **GRAIN:** Monthly\n",
    "- **GROUP BY:** Department\n",
    "- **BASELINE:** Seasonal-Naive (`ŷ_t = A_{t-12}` within each group)\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "- Compute portfolio-ready accuracy metrics for **TEST_YEAR** by **Department**.\n",
    "- Benchmark the **Provided Forecast** against a **Seasonal-Naive** baseline.\n",
    "- Produce tidy outputs for dashboards and the README.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs\n",
    "- `data/processed/monthly_sim.csv`\n",
    "\n",
    "**Required columns**\n",
    "- `Month` (date or parseable string, month start preferred)\n",
    "- `Year` (int)\n",
    "- `Department` (str)\n",
    "- `Actual` (float)\n",
    "- `Forecast` (float)  <!-- Provided forecast from 3 month rolling  mean-->\n",
    "- `Budget` (float), `ShockFlag` (bool/int)\n",
    "\n",
    "---\n",
    "\n",
    "## Metrics (percent)\n",
    "- **MAPE:** mean(|A−F| / |A|) × 100  *(skip terms where A = 0)*\n",
    "- **WAPE:** sum(|A−F|) / sum(|A|) × 100\n",
    "- **sMAPE:** mean( |A−F| / ((|A|+|F|)/2) ) × 100  *(skip terms where denom = 0)*\n",
    "\n",
    "---\n",
    "\n",
    "## Method\n",
    "1. Load monthly data; enforce one row per `(Department, Month)`.\n",
    "2. Create **Naive** = `Actual.shift(12)` within each `Department`.\n",
    "3. Filter to `Year == TEST_YEAR`.\n",
    "4. For each `Department`, compute MAPE/WAPE/sMAPE for:\n",
    "   - **Provided**: `Forecast` vs `Actual`\n",
    "   - **Naive**: `Naive` vs `Actual`\n",
    "5. Save results and a simple comparison chart.\n",
    "\n",
    "---\n",
    "\n",
    "## Outputs\n",
    "- `outputs/metrics/accuracy_{TEST_YEAR}_by_department.csv`\n",
    "- `outputs/figures/accuracy_wape_{TEST_YEAR}.png`\n",
    "\n",
    "---\n",
    "\n",
    "## Acceptance Checklist\n",
    "- [ ] 12 months present for every department in **TEST_YEAR**  \n",
    "- [ ] `Naive[t] == Actual[t−12]` (spot-check a few months per dept)  \n",
    "- [ ] Metric functions pass a toy sanity test (e.g., A=[100,100], F=[90,110] → ~10%)  \n",
    "- [ ] CSV written with Provided vs Naive side-by-side  \n",
    "- [ ] One-sentence executive takeaway added to README/dashboard\n",
    "\n",
    "---\n",
    "\n",
    "> **Notes**\n",
    "> - If `ShockFlag` exists, consider a second table isolating shock vs non-shock months.\n",
    "> - Prefer year-tagged **outputs**, but keep the notebook name generic with a `TEST_YEAR` parameter."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
